{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from keras.layers import Embedding, Dense, Permute, RepeatVector\n",
    "from keras.layers import Lambda, Conv1D, Dropout, Activation, Multiply, Flatten\n",
    "from keras.models import Sequential, Input, Model\n",
    "from keras.models import K\n",
    "\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"paper\")\n",
    "sns.set_style(\"ticks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=10\n",
    "emb_size=3\n",
    "num_filters=6\n",
    "kernel_size=3\n",
    "maxlen=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        pre_tokens=(\"__PAD__\", \"__BOS__\", \"__EOS__\", ),\n",
    "        UNK=\"__UNK__\",\n",
    "        preprocess=lambda x: x\n",
    "    ):\n",
    "        self.UNK = UNK\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "        self.offset_tokens = set((self.UNK, ) + pre_tokens)\n",
    "        self.preprocess = preprocess\n",
    "        for token in pre_tokens + (self.UNK, ):\n",
    "            self.add_token(token)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        if token not in self.offset_tokens:\n",
    "            token = self.preprocess(token)\n",
    "        if token not in self.word2idx:\n",
    "            self.word2idx[token] = len(self.word2idx)\n",
    "            self.idx2word.append(token)\n",
    "\n",
    "    def get_word2idx(self, token):\n",
    "        if token not in self.offset_tokens:\n",
    "            token = self.preprocess(token)\n",
    "        return self.word2idx.get(token, self.UNK)\n",
    "\n",
    "    def process_seq(self, seq):\n",
    "        return [self.get_word2idx(token) for token in seq]\n",
    "\n",
    "    \n",
    "    \n",
    "def generate_vocab(documents):\n",
    "    \"\"\"Generate vocab from list of documents\n",
    "    \n",
    "    Args:\n",
    "    -----\n",
    "        documents: list of document where each document is a list of words\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "        vocab: Vocab object\n",
    "    \"\"\"\n",
    "    vocab = Vocab()\n",
    "    for document in (documents):\n",
    "        for word in document:\n",
    "            vocab.add_token(word)\n",
    "    return vocab\n",
    "\n",
    "def create_sequences(documents, labels, vocab, maxlen):\n",
    "    \"\"\"Create sequences for keras models\n",
    "    \n",
    "    Args:\n",
    "    -----\n",
    "        documents: list of document where each document is a list of words\n",
    "        labels: list of labels per document. Only binary classification is supported, i.e. 0, 1\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "        \n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = np.asarray(labels)\n",
    "    for document in (documents):\n",
    "        seq = []\n",
    "        for word in document:\n",
    "            seq.append(word)\n",
    "        X.append(vocab.process_seq(seq))\n",
    "    X_padded = sequence.pad_sequences(X, maxlen=maxlen)\n",
    "    return X_padded, y\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic model which takes sequence and outputs sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_model():\n",
    "    model = Sequential()\n",
    "    # we start off with an efficient embedding layer which maps\n",
    "    # our vocab indices into embedding_dims dimensions\n",
    "    model.add(Embedding(vocab_size,\n",
    "                        emb_size,\n",
    "                        input_length=maxlen))\n",
    "    model.add(Dropout(0.2))\n",
    "    # we add a Convolution1D, which will learn filters\n",
    "    # word group filters of size filter_length:\n",
    "    model.add(\n",
    "        Conv1D(\n",
    "            num_filters,\n",
    "            kernel_size,\n",
    "            padding='same',\n",
    "            activation='relu',\n",
    "            strides=1\n",
    "        )\n",
    "    )\n",
    "    return model\n",
    "    \n",
    "def get_attention_model():\n",
    "    attention = Sequential()\n",
    "    attention.add(Dense(num_filters, input_shape=(maxlen, num_filters), activation=\"tanh\"))\n",
    "    attention.add(Dense(1))\n",
    "    attention.add(Flatten())\n",
    "    attention.add(Activation(\"softmax\"))\n",
    "    return attention\n",
    "\n",
    "def get_output(base_model, attention, inputs):\n",
    "    activations = base_model(inputs)\n",
    "    permited_activations = Permute((2,1))(activations)\n",
    "    aligned_attention = RepeatVector(num_filters)(attention(activations))\n",
    "    final_activation = Multiply()([\n",
    "        permited_activations,\n",
    "        aligned_attention\n",
    "    ])\n",
    "    final_score = Flatten()(final_activation)\n",
    "    output = Lambda(lambda x: K.sum(x,-1, keepdims=True))(final_score)\n",
    "    output = Activation(\"sigmoid\")(output)\n",
    "    return output\n",
    "\n",
    "def get_model():\n",
    "    inputs = Input((maxlen,))\n",
    "    base_model = get_base_model()\n",
    "    attention = get_attention_model()\n",
    "    output = get_output(base_model, attention, inputs)\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 5)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 5, 6)         90          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_2 (Sequential)       (None, 5)            49          sequential_1[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "permute_1 (Permute)             (None, 6, 5)         0           sequential_1[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 6, 5)         0           sequential_2[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 6, 5)         0           permute_1[0][0]                  \n",
      "                                                                 repeat_vector_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 30)           0           multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1)            0           flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 1)            0           lambda_1[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 139\n",
      "Trainable params: 139\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 5), (100, 1))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_size=100\n",
    "X = np.random.randint(vocab_size, size=(data_size, maxlen))\n",
    "y = np.random.randint(2, size=(data_size,1))\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s 24ms/step - loss: 0.6960 - acc: 0.4600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2533848c128>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit model on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    [\"The\", \"world\", \"is\", \"on\", \"fire\"],\n",
    "    [\"The\", \"earth\", \"is\", \"on\", \"fire\"],\n",
    "    [\"The\", \"country\", \"is\", \"on\", \"ice\"],\n",
    "    [\"The\", \"book\", \"is\", \"on\", \"fire\"],\n",
    "    [\"The\", \"cat\", \"is\", \"on\", \"trampoline\"],\n",
    "]\n",
    "\n",
    "labels = [0,0,1,0,1]\n",
    "vocab = generate_vocab(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = create_sequences(documents, labels, vocab, maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 5), (5,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "\r",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6967 - acc: 0.4000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2533848c278>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.topology.InputLayer at 0x2533848c390>,\n",
       " <keras.models.Sequential at 0x2533894f630>,\n",
       " <keras.models.Sequential at 0x25333c1b550>,\n",
       " <keras.layers.core.Permute at 0x253385d5e48>,\n",
       " <keras.layers.core.RepeatVector at 0x253385c1f98>,\n",
       " <keras.layers.merge.Multiply at 0x2533866ba58>,\n",
       " <keras.layers.core.Flatten at 0x2533861dbe0>,\n",
       " <keras.layers.core.Lambda at 0x253386a8390>,\n",
       " <keras.layers.core.Activation at 0x2537fe20c18>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'conv1d_1/Relu:0' shape=(?, 5, 6) dtype=float32>,\n",
       " <tf.Tensor 'activation_1/Softmax:0' shape=(?, ?) dtype=float32>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1].get_output_at(0), model.layers[2].get_output_at(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding_1_input:0' shape=(?, 5) dtype=float32>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1].get_input_at(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_activation = K.function(\n",
    "    [\n",
    "        model.layers[1].get_input_at(0),\n",
    "        K.learning_phase()\n",
    "    ],\n",
    "    [model.layers[1].get_output_at(0)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.backend.tensorflow_backend.Function at 0x253290c7390>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[ 0.        ,  0.        ,  0.01670015,  0.        ,  0.01758255,\n",
       "           0.        ],\n",
       "         [ 0.01330368,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ],\n",
       "         [ 0.        ,  0.02499684,  0.        ,  0.        ,  0.        ,\n",
       "           0.03441959],\n",
       "         [ 0.00910494,  0.        ,  0.00106554,  0.        ,  0.02439762,\n",
       "           0.        ],\n",
       "         [ 0.        ,  0.01371591,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ]]], dtype=float32)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_activation([\n",
    "    [X[0]],\n",
    "    0.\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dense_1_input:0' shape=(?, 5, 6) dtype=float32>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[2].get_input_at(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_activation = K.function(\n",
    "    [\n",
    "        model.layers[2].get_input_at(0),\n",
    "        K.learning_phase()\n",
    "    ],\n",
    "    [model.layers[2].get_output_at(0)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.20158802,  0.19940449,  0.19962992,  0.19902216,  0.2003554 ]], dtype=float32)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_activation([\n",
    "    conv_activation([[X[0]],0.])[0],\n",
    "    0.\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations(model, model_input):\n",
    "    \"\"\"Taken from:\n",
    "    https://github.com/philipperemy/keras-visualize-activations/blob/master/read_activations.py\n",
    "    \"\"\"\n",
    "    conv_activation_fn = K.function(\n",
    "        [\n",
    "            model.layers[1].get_input_at(0),\n",
    "            K.learning_phase()\n",
    "        ],\n",
    "        [model.layers[1].get_output_at(0)]\n",
    "    )\n",
    "    conv_activations = conv_activation_fn([\n",
    "        [model_input],\n",
    "        0.\n",
    "    ])\n",
    "    attention_activation_fn = K.function(\n",
    "        [\n",
    "            model.layers[2].get_input_at(0),\n",
    "            K.learning_phase()\n",
    "        ],\n",
    "        [model.layers[2].get_output_at(0)]\n",
    "    )\n",
    "    attention_activations = attention_activation_fn([\n",
    "        conv_activations[0],\n",
    "        0.\n",
    "    ])[0]\n",
    "    return attention_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_activations = get_activations(model, X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_activations(model, model_input, xticklabels):\n",
    "    attention_activations = get_activations(model, X[0])\n",
    "    ax = sns.heatmap(attention_activations, xticklabels=xticklabels, square=True)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x25328d94ac8>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x253291b8198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_activations(model, X[0], documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
